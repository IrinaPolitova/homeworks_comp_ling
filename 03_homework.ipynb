{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c678e33-7efc-47da-bd84-890daf4b5beb",
   "metadata": {},
   "source": [
    "# Задание 1 (5 балла)\n",
    "\n",
    "Имплементируйте алгоритм Леска (описание есть в семинаре) и оцените качество его работы на датасете `data/corpus_wsd_50k.txt`\n",
    "\n",
    "В качестве метрики близости вы должны попробовать два подхода:\n",
    "\n",
    "1) Jaccard score на множествах слов (определений и контекста)\n",
    "2) Cosine distance на эмбедингах sentence_transformers\n",
    "\n",
    "В качестве метрики используйте accuracy (% правильных ответов). Предсказывайте только многозначные слова в датасете\n",
    "\n",
    "Контекст вы можете определить самостоятельно (окно вокруг целевого слова или все предложение). Также можете поэкспериментировать с предобработкой для обоих методов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5814a100-9ba8-4787-b0ac-73fa657a3fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\irina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# подгружаем библиотеки\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c70deda-7634-49b8-8b58-dd9e869a2af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# подгружаем корпус\n",
    "\n",
    "corpus_wsd = []\n",
    "corpus = open('corpus_wsd_50k.txt').read().split('\\n\\n')\n",
    "for sent in corpus:\n",
    "    corpus_wsd.append([s.split('\\t') for s in sent.split('\\n')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a620415b-c942-4f83-856b-b4f02663401e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34106947101657686\n"
     ]
    }
   ],
   "source": [
    "# сначала попробуем Jaccard\n",
    "\n",
    "correct_guesses = 0 # счетчик угаданных значений\n",
    "all_guesses = 0 # счетчик попыток угадать значение\n",
    "\n",
    "for sentence in corpus_wsd: # бежим по предложениям\n",
    "    for word in sentence: # внутри предложения бежим по словам\n",
    "        if word[0] != '': # рассматриваем только многозначные слова - у которых в первой ячейке что-то написано\n",
    "            sentence_string = '' # здесь будет строкой лежать контекст - всё предложение, в котором целевое слово заменено на нижнее подчеркивание\n",
    "            for word_1 in sentence:\n",
    "                if word_1 != word:\n",
    "                    sentence_string += word_1[2] + ' ' # для всех слов, кроме целевого, прибавляем их в контекст\n",
    "                else:\n",
    "                    sentence_string += '_ ' # целевое слово не добавляем, добавляем вместо него нижнее подчеркивание\n",
    "\n",
    "            # на этом этапе мы для данного слова имеем контекст, например: \"What a _ day!\" для слова word = \"beautyful\"\n",
    "            # далее будем доставать все возможные определения для этого слова и сравнивать каждое из них с контекстом:\n",
    "            \n",
    "            all_jaccards = [] # здесь будет лежать список всех Жаккаров\n",
    "            \n",
    "            for synset in wn.synsets(word[1]): # идем по синсетам этого слова (здесь берем word[1], поскольку там лежит лемма слова)\n",
    "                new_definition = synset.definition() # вытаскиваем определения этого синсета\n",
    "\n",
    "                # считаем метрику Жаккара для этого определения и контекста - контекст лежит в sentence_string:\n",
    "                \n",
    "                new_intersection = (set(new_definition.split()) & set(sentence_string.split()))\n",
    "                new_union = (set(new_definition.split()) | set(sentence_string.split()))\n",
    "                new_jaccard =  len(new_intersection) / len(new_union) # посчитали метрику Жаккара сюда\n",
    "                all_jaccards.append([synset, new_jaccard]) # добавили в список Жаккаров сам синсет + метрику Жаккара для него\n",
    "\n",
    "            # на этом этапе у нас есть список вида [[synset1, jaccard1], [synset2, jaccard2], ...]\n",
    "            # далее будем выискивать тот синсет / те синсеты, при котором / которых метрика Жаккара принимает наибольшее значение\n",
    "            # важно: таких синсетов может быть несколько, и я бы хотела учесть их все, а не взять какой-то один случайный из них\n",
    "\n",
    "            max_synsets = [] # список таких синсетов\n",
    "            \n",
    "            max_jaccard = all_jaccards[0][1] # сюда положим предварительное максимальное значение Жаккара\n",
    "            \n",
    "            for jaccard in all_jaccards:\n",
    "                if jaccard[1] == max_jaccard: # если нашли что-то с таким же Жаккаром, добавляем синсет в список max_synsets\n",
    "                    max_synsets.append(jaccard[0])\n",
    "                elif jaccard[1] > max_jaccard: # если нашли что-то с Жаккаром побольше, то меняем max_jaccard и полностью заменяем max_synset\n",
    "                    max_jaccard = jaccard[1]\n",
    "                    max_synsets = []\n",
    "                    max_synsets.append(jaccard[0])\n",
    "\n",
    "            # на этом этапе мы знаем синсет(ы) с наибольшим значением метрики Жаккара\n",
    "            # нам осталось сравнить эти синсеты с word[0] - то есть с тэгом многозначного слова, с которым мы работаем\n",
    "            \n",
    "            list_of_keys = [] # это список тэгов (поскольку у одного синсета может быть несколько лемм, а потому несколько тэгов)\n",
    "\n",
    "            for synset in max_synsets: # идем по всем синсетам с максимальным значением Жаккара\n",
    "                for lemma in synset.lemmas(): # идем по леммам каждого такого синсета\n",
    "                    list_of_keys.append(lemma.key()) # для каждой леммы вытаскиваем ее тэг\n",
    "\n",
    "            # сейчас у нас есть список тэгов данного синсета, и нам нужно понять, есть ли среди этих тэгов правильный - совпадающий с word[0]\n",
    "            \n",
    "            is_correct = 0 # пока что будем считать, что правильного тэга нет, но если найдем, поменяем значение переменной\n",
    "            \n",
    "            for k in list_of_keys:\n",
    "                if k == word[0]:\n",
    "                    is_correct += 1 # если нашли тэг, совпадающий с word[0], меняем значение переменной\n",
    "\n",
    "            if is_correct > 0: # если значение переменной is_correct поменялось, значит, мы угадали!\n",
    "                correct_guesses += 1 # добавляем единичку в счетчик угаданных значений\n",
    "\n",
    "            all_guesses += 1 # вне зависимости от правильности угадывания, увеличиваем счетчик попыток угадать значение\n",
    "\n",
    "\n",
    "accuracy = correct_guesses / all_guesses # значение accuracy\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0105f6c-c5d0-4d21-9b9b-4395f96fbd98",
   "metadata": {},
   "source": [
    "Таким образом, при использовании метрики Жаккара точность получилась 0.34106947101657686, то есть примерно 34,11%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d79cbc92-3505-47c1-b3cd-dc110c156c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# подгружаем библиотеки\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_distances, cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "154e67a8-3b81-4ec4-ad73-16350d424f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# подгружаем модель\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "embed = model.encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e1d0ed-cab4-462c-9959-857880c9fa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# теперь будем пробовать Cosine distance\n",
    "# сначала многие действия будут аналогичны тем, которые мы делали в случае с метрикой Жаккара\n",
    "\n",
    "correct_guesses = 0 # счетчик угаданных значений\n",
    "all_guesses = 0 # счетчик попыток угадать значение\n",
    "\n",
    "for sentence in corpus_wsd: # бежим по предложениям\n",
    "    for word in sentence: # внутри предложения бежим по словам\n",
    "        if word[0] != '': # рассматриваем только многозначные слова - у которых в первой ячейке что-то написано\n",
    "            sentence_string = '' # здесь будет строкой лежать контекст - всё предложение, в котором целевое слово заменено на нижнее подчеркивание\n",
    "            for word_1 in sentence:\n",
    "                if word_1 != word:\n",
    "                    sentence_string += word_1[2] + ' ' # для всех слов, кроме целевого, прибавляем их в контекст\n",
    "                else:\n",
    "                    sentence_string += '_ ' # целевое слово не добавляем, добавляем вместо него нижнее подчеркивание\n",
    "\n",
    "            # на этом этапе мы для данного слова имеем контекст, например: \"What a _ day!\" для слова word = \"beautyful\"\n",
    "            # далее будем доставать все возможные определения для этого слова и сравнивать каждое из них с контекстом:\n",
    "            \n",
    "            all_cosines = [] # здесь будет лежать список всех косинусных расстояний\n",
    "            \n",
    "            for synset in wn.synsets(word[1]): # идем по синсетам этого слова (здесь берем word[1], поскольку там лежит лемма слова)\n",
    "                new_definition = synset.definition() # вытаскиваем определения этого синсета\n",
    "                definition_emb = embed(new_definition) # делаем эмбеддинг определения\n",
    "                context_emb = embed(sentence_string) # делаем эмбеддинг контекста\n",
    "\n",
    "                # считаем косинусное расстояние:\n",
    "                \n",
    "                new_distance = cosine_distances(context_emb.reshape(1, -1), definition_emb.reshape(1, -1))\n",
    "                all_cosines.append([synset, new_distance[0][0]]) # добавили в список косинусных расстояний сам синсет + расстояние\n",
    "\n",
    "            # на этом этапе у нас есть список вида [[synset1, distance1], [synset2, distance2], ...]\n",
    "            # далее будем выискивать тот синсет / те синсеты, при котором / которых косинусное расстояние минимально\n",
    "            # важно: таких синсетов может быть несколько, и я бы хотела учесть их все, а не взять какой-то один случайный из них\n",
    "\n",
    "            min_synsets = [] # список таких синсетов\n",
    "            \n",
    "            min_cosine = all_cosines[0][1] # сюда положим предварительное минимальное значение метрики\n",
    "            \n",
    "            for cosine in all_cosines:\n",
    "                if cosine[1] == min_cosine: # если нашли что-то с таким же расстоянием, добавляем синсет в список min_synsets\n",
    "                    min_synsets.append(cosine[0])\n",
    "                elif cosine[1] < min_cosine: # если нашли что-то с расстоянием поменьше, то меняем min_cosine и полностью заменяем min_synset\n",
    "                    min_cosine = cosine[1]\n",
    "                    min_synsets = []\n",
    "                    min_synsets.append(cosine[0])\n",
    "\n",
    "            # на этом этапе мы знаем синсет(ы) с наименьшим косинусным расстоянием\n",
    "            # нам осталось сравнить эти синсеты с word[0] - то есть с тэгом многозначного слова, с которым мы работаем\n",
    "            \n",
    "            list_of_keys = [] # это список тэгов (поскольку у одного синсета может быть несколько лемм, а потому несколько тэгов)\n",
    "\n",
    "            for synset in min_synsets: # идем по всем синсетам с минимальным значением метрики\n",
    "                for lemma in synset.lemmas(): # идем по леммам каждого такого синсета\n",
    "                    list_of_keys.append(lemma.key()) # для каждой леммы вытаскиваем ее тэг\n",
    "\n",
    "            # сейчас у нас есть список тэгов данного синсета, и нам нужно понять, есть ли среди этих тэгов правильный - совпадающий с word[0]\n",
    "            \n",
    "            is_correct = 0 # пока что будем считать, что правильного тэга нет, но если найдем, поменяем значение переменной\n",
    "            \n",
    "            for k in list_of_keys:\n",
    "                if k == word[0]:\n",
    "                    is_correct += 1 # если нашли тэг, совпадающий с word[0], меняем значение переменной\n",
    "\n",
    "            if is_correct > 0: # если значение переменной is_correct поменялось, значит, мы угадали!\n",
    "                correct_guesses += 1 # добавляем единичку в счетчик угаданных значений\n",
    "\n",
    "            all_guesses += 1 # вне зависимости от правильности угадывания, увеличиваем счетчик попыток угадать значение\n",
    "\n",
    "\n",
    "accuracy = correct_guesses / all_guesses # значение accuracy\n",
    "print(accuracy)"
   ]
  },
  {
   "attachments": {
    "1a128d63-9bd7-4c58-aa81-cad327627ef0.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUIAAACaCAIAAABT1dY9AAAgAElEQVR4Ae1d/28TR9rvv/H+AZsfoggJaX9ACUIoVBGKAJ0Boe6hcnZ7gJMATqGHC5czFIiSgiyo6gZUuRSd73Q6lh6SCSXukeMWXZCD4Bpd4VIIkDflrSEl2Rw0CaSG+L2Z2Z2dnf1ix7FDbD/7QzL77ZlnPvN8dp55drzPGxnYAAFAoMQReKPE9Qf1AQFAIAM0BiMABEoeAaBxyXchNAAQABqDDQACJY8A0LjkuxAaAAgAjcEGAIGSRwBoXPJdCA2gCPQWaKMCS6UANC6VngI9syNQIBb3Zq9pkV0BNF5kHQLqzAMBoPE8wINbAYHFgYAtjS9cuPDpp58eOHCgtbV1G95aW1sPHDjQ1dV14cIF21sWR2vmoEXBRuNHjx4dO3ZsenqarXxycrKzs/PHH39kD0IZECgSAhwne3p6Ojs73333XZ/D9u677x49erSnp4e7sUjqFU9swWjc2dnp8/kOHjxImTw9PX3w4EGfz/fRRx8VrwEgGRCgCLBs7O7u3r17N+Hvrl27AoFAc3Pz1q1bt23b1tzcHAgEdu3aRc7u3r374sWL7L1UYKkUCkbjiYmJ3/72tz6f77333iPokEIoFHr69Okih0OW5Y0bN6qqusj1ZNV7+PChx+Pp6upKp9Ps8fzK/f39K1euvHfvXn63F/yu/PShVPzqq6927tzp8/m2b99OyWwdkt97773t27f7fL5du3ZdunSJ3l7w5hRbYMFoPDk5SWjMgRUKhSYnJwvbjP7+/ipmE0Wxra1taGgo71oWnsbpdPrvf/+7z+erqampqqpqaGg4ffr01NSUexNmZ2fPnTu3ZcuW0dHRBw8erFmzpuxpPDk5uXfv3k8++SSXpxXh4eXLl/ft2+fz+YLBYEtLC2eQ3G5LS8sHH3zg8/n27dt3+fJlIsG9Fxbh2cLQ+OXLl4cPH+YAoruHDx9+9epVARtPaHz69Om+vr6vv/66s7NzxYoVoiheu3Ytl1quX7++f//+iYkJevHC03h6erq9vZ004erVq0ePHq2pqQkGg3RKQnVjC3fu3GlsbPzmm2/YgwUp5zf6FaRqWyGsPvfv31+3bl1/f7/tlexBQsJPP/3U5/M1Nzc3NTVRI6SFd955h5ZJobm52e/3+3y+rq6uCqUxh0jWXRb0vMuExmy/jo6Obt68uaWlJZeR30pa65G8dcvvxtnZ2d///vfLli27ffu2k4SZmZnf/e53HR0duYxLTkKcjrO0cbpmIY+z+hBwcunc3t7eS5cubdu2zefzufjSVivds2cP8cBJuGshW1qQuuY7GlsRcT9SEKWtNM5kMn/+859Xr1798OHDrFVYSWs9klVIwS+wbRRby+Dg4Jo1awYHB9mDhSqztCmUzPnI4fQZGRlZu3btzZs33WX29vZ+9tlnZK7rbofWsyTiFY1Ge3srdfnHr3/9a4oLCzQ9uG3bNvb4PMu2Fi/L8qpVq27dutXS0uL1erm4mqIooijG4/GVK1cy0+qqY8eOZTIZQuPHjx9/+eWXDQ0NVVVVHo/n6tWrs7OzVNWpqanTp0+vXbu2qqqqpqbG5/Ndu3aNXqCq6saNG2VZ/v7771tbW2tqakRR7Ozs/Omnn6gE90J3d7f7Y+jMmTNNTU3U3aA1ErHE7oeGhq5du/b2229XV1fX1taeOHGCxO0GBweXL19+8uRJVod0Ot3R0bFly5aJiQmONsS72bBhw/3798ktqqqeOHGitraWgNPd3U2dAoLe/fv3jxw5UlNT4wLp9evXKWKZTMZFJqfPzz//3NbWFg6H2dvZtpDyfxl44MABn88XCASo7eVYaG1t9fl8H374YeXSmEWKBdfpOHtNHmUrjWdmZvbt20fY293dvXTp0oGBASqZGEFLS8vjx4+TyWRnZ2dDQ0NPT09fX9+dO3cIjdesWbN///5QKHT16tWenp4NGzYsX7781q1bRMjo6KjP56utrT158iSZkG/durW6uvrcuXPEsAip2trafvnLX54/f76vry8cDldXV+fiA09OTvb09Lz55pvnz593MtMXL17s3buX5aEtjT///PPNmzfH4/G+vr6TJ0+Korh3797JyUkWHwrLw4cPV69e/Yc//CGTybC0UVW1tbW1sbGRRg0Jq9etW0cknzhxoqam5tSpU0RbWZYbGhr27Nnzz3/+k+ovy7ItpNSbcJfJ6kMUlmXZ+nSmbSGF3t5eMqjazopZa7SWyS2tra2VSGMrHO5HONzz22VpPDs7Ozo62tHRsWTJkq+++iqTyYyOjq5fv76rq4ua1MjISENDQ3d3N6nO6kLLslxVVfXxxx/TEebWrVt0+Eqn08ePH29oaLh79y5VOJ1Of/zxxw0NDSMjI2Rg2bhxI3tNOp0+cuSIywA7PT29e/du4hqsXbv222+/pcKtBULaS5cu0VNWGldVVbW2ttLhOpPJXLlypbq6mtxF/BHWL+3u7qb6U9qQyDDXkOPHj7/11lujo6Ok9tnZ2bNnz9KmEfQovOSaXCB1kUn1oe3t7+9ftWrVgwcP6BFrobe3l7xA2rp1q5MdWkNc5EpyY1NTE9DYCTrjuBX6PI4QGrO+cW1tbTweJ/Hw2dnZcDhMfEUiXJbl9evXUyu0pfGKFStYlk5OTjY1NRH/8PHjx+vWrWOfC0TsvXv3Vq5cef78eUrjzs5O+uzIZDKXLl3ixLKNTafT//rXv8jYfvDgwaVLl4bD4efPn7PX0DIZOdmoni2Nr1y5Qm/JZDJPnz71er1Eq/Hx8U2bNtFWkIcIdRYIbb799tuPPvqooaGBjpmZTIY0/8yZM6zku3fvrlixgugjy7K1mdaDrDJZZVppfO/evQ0bNri/2e7t7W1ubibBKsPmciuRYHVzc3Ml0ph0LQsU29lOx9lr8igTGpO3NX19fd988w1n/Tdv3hRFkRjZ5ORkS0sLO62ypfH69evHxsaoMsTKd+/ePT09TejKMYRS9/jx47QsyzKVQDzVqqoqlnvsWa587do1URQ5CfQaogMrykpjK5fYVszOznZ1dW3atGl8fDyTyQwODlIeElWXLVv2q1/9avny5devX6f1ZjIZUjX70KRloi15SrLo0XADu6KGVSarzLxpTALUecyNyS179uypXBqzPgxrAZTG27dvZ4/Ps8w61baiCHXJUHP79u2VK1eyzqQtjblVXFabKzaNp6amWltb9+7d++LFC2ujHjx4sGrVKncaW5dhkVa0tbX9/PPPlLqKomQymTNnzrAOS39//4oVKzo6OpYtWxaNRunkgtL4s88+67Ns5L2AFc8caewiM28ak/ULZEUHNb9cCuSWI0eOVCKNcwGIvcZqoHkcyUrjTCbT3d29fv36x48fd3V1ca8crWZnPcLS2N2pJjNPbmwkjcpFT9p8QmNKOXqcFIh89lHC1djf319dXc3ynPrDNDBGAl0dHR1jY2NbtmwhwS2q6sqVK4eGhs6dO7dkyZKzZ8/SFTvEn+ecalY9K3pZaZxVppXGN27caGhoyDo3/vzzz30+H1mJyRpe1vKOHTt8Pt/p06eBxlmx8rHdn3c5F3qQsNbZs2c3bdpkjb5wwRKrIbI0Ji9m2KhPJpMhIa7GxkY2xMW5xLnoSUEgTjWnKj2bS6S6qqrq0KFDMzMz5C6yakIURXbVV3d397p16/7yl7+sXr2aaE4uprRJp9OffPLJkiVLaBCekJ8NR1GtSMGKXlYaZ5VJ9aF1nT9/PpdI9ddff01izvSXD9mNUn9B1dTU9Ne//rUSaUxQTqfToVDICa8PP/zw5cuXtD/mX8iFHoR79fX1VvsjA9ehQ4euXr164cKFrDZHot+bN2/mXjixyz+5sZG00UXPGzdukB/W9PX1Xbly5fDhwzU1Nfv372fjzBxQZ86cYd0Krsb+/n4yuW1ubr5y5cp/X3qHQqHq6mo2/E4asmHDhtWrV9PgFlWV+uTkCcW+Trt7925DQ8Obb775pz/9qa+v7+rVq6dOnWpvbydLR/OgcSaTcZfJ0TidTh86dIjTmcMnk8mQpZRdXV0+n2/r1q0k3OVklvR4S0sLmRWeOnWqQhdjUijHx8dtfxrR1tZGYir0yvkXXOjBCr958+bSpUutfZ9Op7/44gtRFGtqaoivaDVEdjQmMp8+fdrV1VVfX19VVWX9MQZHKnKLi56PHz9ua2sjqymqq6vffvvt3t5eOpCyraBlbhUXVyOx+3//+99//OMfV6xYYV2kQeSQQFd1dTWZIVPhHG0mJyf3799fU1NDX2V///33bW1toihWVVXV1ta2trbeuHGDvjfmIgu5PBkzmYyLTE6fkZGR1atXczpT5WmBkPDy5cvBYNDn833wwQdZmbxz504yK4afRmgwPn/+/NKlSx0dHU1NTTt27Ojs7EwkErYBG4p7UQs3b96sra1lg1tFra7Ywsma6uPHj7PxJ1opZ/f0OFcgNGaDW9wFi3CXzA6am5ufPXvmrh6hcW9vL/2h4o4dO95//3068HKFvXv3kinxzp074YeK7ti+nrPEqWa90NejR0FrdfmFU440Jmtj2OBWQRUsirC5/sKJkPnixYvk5dM777xz4MCB999/f8eOHVvx1tLS8pvf/IYs2yQ/ooDPBhSl5+YvlFu5NX+Bi0EC+3tjTp8cacyu3OIkLM7dPH5vTMfknp6eo0ePun/E59ixY/ARn8XY9RcuXLh48aIkSdzKxMWoa+F0cqfxxMTEl19+eebMmdra2rNnz7JLzQqnwuuXRAnMFuLxOHxS7/X3zVw1OH78eHV1dWtrK7euaK5ySuv6rDR+6623RFH84osvbKfWpdVYJ21Z9s6n7CR/0R6f7++NF23DQDFAoHIQABpXTl9DS8sWAaBx2XYtNKxyEAAaV05fQ0vLFgGgcdl2LTSschAAGldOX0NLyxYBoHHZdi00rHIQcKTxq1evZmZmnj9/Pg0bIFCOCDx//rxsXqHb0HhqampiYkKFDRCoDASePXtW2B/SLrwX8Aa72OXatWupVKoy+g5aCQgYCIyPj9++fftvf/sbS4cSKr/xWN+ePHliNAtKgEDlITA6Ovrdd9/dvn3733gbxNt3xdyG5r3du3dveHj4jSm8TU5OVl6vQYsBAR6BR48ejYyMfI+3h3j7v2JujwqxjY6OvvESbz/99BPfINgHBCoSgVF9+xFvT4q5FQTgiYkJFOJ69epVQcSBEECgDBAY07dxvBW1RU8LtCEaz8zMFFVXEA4IlBAC4+PjhMglRuOpqakSQhlUBQSKjUBJ0vjp06fFxgXkAwIlhEBJ0vjZs2clBDGoCggUGwGgcbERBvmAQNERABoXHWJVCQuCVx4ufkVQQ6UiUHY0RpwRwoqpPwdlvyiI/tiA6eiC7ZQhjYdlr8BsYr0UiCjwoFowk+IrWlQ0/sUvfpH1tRR64eQ2N7bSWAl7BMHDMZvHoZj7+dI4pUSDUiQ5B9UGE6FAML4AdMI0ltrjiqIoCTnaHpTqBEH0L0TVc4Cjci5dPDT+hb65M3mONNYGYnnwNXZpvjTGXJnT4yffmuYMDlbNNFEYRONz45weOnOuFW5wQmCR0FinsPbfhclzojEaiEX/a+WwquY9Ny4pGuNW8rMZJ6uD4wVGYDHQmOMw2XVicu40xgOxvTM9EA8HPHVoclfnCYTjzJyZjGeDKSXirxe1yJROp9SAHJLQUXRXNMn9QDKbTGdXF91J5CLBESWlqrhKZvJpTPdTNlcjm0CKmzYyVtqNz9wx5Lt7CRpivRSME8dlOO4XRUeX3joap+IBQQgmdFC4OrDVcsds6h2MSXZDutIuwkDvSvzXTmNbDrswOUcaYw7XBTSbNEFA5sqBKJ7YxSPeOkHwUGcQm1ow6G1PGLTDNhsMhyUpFEugyWAs6BEEQYoZnnp2mU6R6mTEI4iaYCUeDUoBfOVwUlHi7ZIgBKIK3gYwQZAqoqTrHpZENOvHs+fUgKIo0YAgaDNWJYkbwFEHA8EeSyWColDnjWAwEnLYX6958XOi8bAS9Ysmt4etQ0efPeZQL34YcK55KhE0g63Lg/8UgddLYxcOOzE5Jxp7PB5B8DI0o+1VlXCj0NjORrLxrE4fR5Cp8U9+MjSaxnXzsJGDTNM80tBGTYZFQTCpY5zE9ZrnxsOJGBqs6YYsnLmdJQq5xnrE5OQPx/3I5TCeR1Sye4FAwoz+UlA2xeLyrtcy8mJmB+Jsq911q8Szr5fGTp6zy/GcaBwMO0WnlZD1yY55rNEF05ijFbZZztaTkUbKn1xkOtEYPw88YRM3dTu0obF+Sv+vqaZ7DlbyWI+YaKwi1piGUV2y+39crz7u66FqUWqn7ci/XiVs8qBRTSLXIe66VeLZ8qRxWFFJiJqf3GHzYwYRo6jxjJifeXTCN5lHRTJ5JffkKFOnGm9lyShy64U6KRhNEM9Zv8KuXlVNDShyNNIe9EqSNqE2HhFW8liPmGmsDiZCyDUX6/1hOfc3v1g1o16ssdLeKIisV8OdN4f6nOtFT0hRhxs95uiODgz85xEoWxrjqI8HvcxkXUZsfvp0U2E3MpU0m5oGlh2dGDvOS6a5H1LJeAS/ehXY6by13sF4oA4H2EKRSAxNZxMRL7tCzEpa6xGOxliRYSUWwiE9UdL5Y1aQ32Oab5xCdemx6vnUi6lLBmBU5KbKRn1QogiUM41VMiSzSz+yhkvszM9KJy2UTIabvGTSDmALw4lQI8NKS73Y3zQ5mHg+YIx6Vu3REZEEwfSabCmITqYGol50NRs40G/i/tvKQHWxNM67XjQdxo40GpjZSCKnBezqCJQ3jY0hWV+ISUKhJiroUOD/ViJolOWMm7XjfGSaKjV2lHYmwIbrYFZGWeNReGbNrNe2khZfYWICf49ROXrsedm3Ruwpc5ltvnYmlQiJhlM9v3oxoqEEemxBcMuMvP1e2dNYH5Kpc40nzUKdNyyjl0d4LWHIH9CXEeZDY72KucjUe0MJo0kxft2jJGJomspwDpmx4GmXFSURjSdVFXNDlNqx5uhqKRg0OdWYXaI/mlAUOUZemaEXR4L+Sikht0t1oVCQDvnDsh9NigkS+OWbGCKDcQ4vnNgQVwBNsE0zgjzr1YBR2kXR42mE4JZuKO7/K4DGqqoOxJAt01dGw2ixMl7HgSaaUiAs07Uc+dFYVdU5ytR7ZVihSz/w7wsS5uhaMoqnrEKdRNyJlBLR161IwVgyxTnVKo5YoYiZWB+iSzHoqhWx3o8Wl7BtTMb0pR9CnccbilEgcqCxER/UVtFwL4XyqlcHBr8KABbrcGT5X3Y0ztJeOF0iCCAaQ3Ar184CGueKFFy3kAig99nM9GIhqy7FuoDGpdhr5a4zDnFBcCv3bgYa544VXFl0BJLxaIJE+rhVc0WvubQrABqXdv+VmfZ4iblY74+aFmiXWSOL0BygcRFABZGAwMIiADReWLyhNkCgCAgAjYsAKogEBBYWAaDxwuINtQECRUAAaFwEUEEkILCwCACNFxZvqA0QKAICQOMigAoiAYGFRQBovLB4Q22AQBEQABoXAVQQCQgsLAJA44XFG2oDBIqAANC4CKCCSEBgYREAGi8s3lAbIFAEBIDGRQAVRAICC4sA0Hhh8YbaAIEiIAA0LgKoIBIQWFgEgMYLizfUBggUAQGgcRFALYpI9D1L/WPuRamgmELxl3eNT32z3+YsZrUVI7vsaIytnfn0Ki4aeRVKt2NzoDHKGEw+3ItSMrFpm83NRp+6snw7NpWMSDlljDDLym0PaJwbTvleVZ405rM0aQma8gVpvvcNJkKBoP5B+3yFZaOxlnsOf4sefVbeOVMi+mq96duxqaSM87EVb7QHGufb7bndV540Nty33FAo8lUF8SHdaYzTfhvZ1lHWm/ZGwTZ1iikx0qCspWALofwTRXPagcbFNTGgcXHxxdKLT2NME+6LsDhVjJ6o1Ggl+gK0caUSxu53iuRZBBobOJVUqXJojMcrPUMR6aNk2GMkD0M5YuJhPbWKJ2CdW6bQeS1pjFgvBeM4UYsdR41jeAxlp+rMNN21OlyZB+VxQamPY8kUluTkZthlSsdJXnhi2k6LMRqu8lmbtocBX+HYoiyjMQNsnSeAEtTANicEKofGqoqzkhmOJxqtaFptNBp5BMETIGnRcFYywbjWkm0tIYf9EfwVVoOyBvDGsdSAoijRgCDoecv0abp7dfisntotIYe9dR6Px9npxSzRcqgZWuCjTEZGlSRzM02LjatzozGXyM6AwR1ANxonIx5BlEIxnA8uHg1KAdkppbuhLZRMCJQnjdnhjzV+bIRkPESGZSJxo9BoypdqShKKTM1I5WZC0KCscZg7xu3idK1u1SnhRi69ujocD4psS4y6UMmegpg7zOivqqZpsVmEgwzzRa4wuLXIhcb409Qm5M11wl4OCJQnjblI9YDhpCFyiiFlMBEUGRKrdk4p5jHxYnF+cDL4WjC1cpTQiuGP5RL36kxJjml96CjvIusnc6SxeVqs36z9t5dhusgFBvcWqS40xlN4TxgcaRPUc9wpTxo7TSERONiXFk0kJlbGDeFkF5MRG6HFZ9WQtnBUGx1daIzlOVbnUJsbzTAVLCMaFmTEuJynxbglbvJJUx0UQyfdW2TJ9G7GLBn1ohgASvScYJ64Gr7wLwcEKo/GJG+32YPGVsgN4Qre8FQWn7awREPXbJLkIHeM2yVG71idQ21IiNNoPBz3o8zj5tzIKnJXjYkworqxpynP/HOTTy5zUAydzAIgOW88XHlAVDWVjEeCEmZzgIQOGdWgmA2BSqMxmh2LwbgS8bBETiWCgkseTnzaiQLIJMWwKecQNmqX0di9OoezyG11ojF2MQTOX8BuB334uE2LsZFkp7ELDA46U+vDiLjRWLtyOBFqRA8kiHFR6HIqVBaNMYlJ7BatjjBiWjgTp7FrgU4JOS6Kwg6tKRUvPsLaooXp7tVhm+eUGYhKyAs3mMCpaNUCN5DO512nxVhWdhqrqjMM7i3KPhrT5tgHBuhpKNgiUJ40tjis2DfGMSvD9UQ2aZCFe5OiJORoyB8wVlDiV0CipL2QUuKxkPbCibzHEuq8kbiiKGgZZF0oFDQNKZiXoj+aUBQ5lsAjjXt16IU28/YrGpCkcDjoRmNVxfdo76hIblEPJX2WaTG2jFxorL1VsoXBtUUuo7ESRpNihJ2iEL1Nz0Rbq4WDHALlSWNL+MgrD1vHKzxLNoisDtNfFghCnUcKhOWkEeJWVXbZQ53HG4wawdUBOUR/koAWLyBKmDzDwUQIT/zE+lBCl+leHV1JgRaaoKpyoBmjhhSSjWgRarrTnIDaQw7y8bUuMDi3yIXGwwq7piYQSXAzfKogFJwRKDsaOze1Ys9knRZXLDJl03Cgcdl0pVNDsk+Lne6E46WCANC4VHoK9AQEHBEAGjtCAycAgVJBAGhcKj0FegICjggAjR2hgROAQKkgADQulZ4CPQEBRwSAxo7QwAlAoFQQABqXSk+BnoCAIwJAY0do4AQgUCoIAI1LpadAT0DAEQGgsSM0cAIQKBUEgMal0lOgJyDgiADQ2BEaOAEIlAoCQONS6SnQExBwRABo7AgNnAAESgUBoHGp9BToCQg4IgA0doQGTgACpYIA0LhUegr0BAQcEQAaO0IDJwCBUkEAaFwqPQV6AgKOCFQUjbnvMzqC8hpOYNVMH9PMokQyKol1zAd4s1wOp0sBgVy/TmppS9nRGCPBf+BW40dZ0TjyGmmcSsr0q7RCHc4FrX+2F1sY+tKtlyRnFsgHerk0EKlkLKRdINZLgYjCnSdfE8YS9A/8sqZr08lzeQSyohZVGWisdwdGgv/cvJZUuHRpnEKf0KYpIPS2vrb/SkQKRuSEgrZ4NCCJKGuskf9mOB7wh2LaF+RRcmZBENk8yyi5FP3GfDwa8KDzbAYqLb0z/oC/ft748r9KUs2ESQVYCUXR80a/NkwKUTHQWEfRDYnSpfEi1hwBjxJw8Cmk9A7RkliyKbIGEuZsa6lEUBREmuMCN5ZNHYEzyxjn+VQyTE0lXnQzXtemladTTS3C3PZFTAasmrNjuIg1RxBn0T5rxgvz/VZjNp9XmczT5g4u9T1ry3NsUWXT2JzIJIDyttANQYp4hf3ZehEl4MWTQHqBquJzxhQvyI4xxm12kzsm0YxY7w8nBrX8UvwUUaeIabKPH1IcsXVt9fwvYr0/SlLX6EeEOpQQhtVea4CetobkmNEuQBmhco+gobwULtll8GhqJFrmdNDbSN1u3LRAnOkLJJ8ZjfXGWgXZH0F9kV834X7SZ/l1HtZCWNtB8/twnAVX756UAb8noPUIVRKLICGEOikYS6Z4GqMMQKR7kPGZzJMKwYVKpjGCmmYVi4e5GR62lagclvT0a/okTZ/EYVdQy8CmJOSwv546ATgtmSi14+kjyS8mMmasndbSuqHJZZ3H45gQdDipKPF2SRD0GT9OzqTbidadWNv2cEAKxRJ6SjPBKyuyv07Ck8hELIimoIwWqpuWudN4UIkFPaJE2262LzU1kAjjYJxLaiZMc9aLRnNj0R8jWaiGE+1oj5kbY2vXnmwkm5bN40/XI/9uUvn0e9FgSNNiMB6oEwQt8R0TH1D0WnH3BMNhiXSIQuBnZxZEui4igUIIHg/KwKcjmYx4BFG7XYlHg1LAOV9sedLYNHgZwHCmP5yIscOvipIcCzQhMLEVIwEj7iA8CSRUsE8Njq7C1ZhvZLOU4tNGtkMsF3UZl71NNwj8n9Ncr0TvcZTpUBAEkclwjO9AkSdqWSoa0wTK4yxamqq32WGoJIUtgWaCAemG+oDFCTDJw08TVk90NpUI1Ru9KEXZkU5FDzayJeQIDrCZI2RIgL7l3U1kwm6KvFGZsteMLDqBm0FdEgK/qVU4FyC9QAk3cnE9dTgeRE6f1mMoDmhYo161w//ypLE+bilk03MLWslgQkUzbO3Jju2U4QW5FFOBUB3lRrIzH9xd/I0obbhAOKoAAAXjSURBVC+Z+mL32eQyIsnOTjWu16o5dwRrSx9BWGJM4lOvs7mDs2hJWuvyNzWg4C0ea0dEqgvETEzTqYbyy3rrBFFqNz0xdcGpgZgf3cxOR1QVj3X6OEVC4aJLnB4TSLBAqtWRZzdhfO1lYuhsTilhFKgjAXvNlkwuCGM7KtsVOhgqPqrTGNfiCdvCZtyhlcqTxswQxLaYM3300B9Q5GikPeiVJG0OQsNMiBjmIRWJ0roHUx1lOxUFAU1uZWY8YsYpYzzBJawWOs06kERBVi6rsla2as4dIdqyvqWNRKyZhk0WLW10cDlERiIH1FEkG+VrpshqglLJqLcO85+ZBWtxbTHI+NCaANH0lDJpg02e+hmmU6payG7Cou17ULMNzfnnugffx/QILvKPes2nojBifAT8ai6hj0Rc2/TdCqaxNr/xBEKRCH7JmYh4GWOzEgNhhvFnDWZYiYX8KAJGp4foRk+7rFg33BdErukxrcvlDV3vJK1a2r9UEeOIVVvGaHQ56CL9YZ9FS/2WXP+j0UUfiWzuwc4GO70dlAN1gido425jSloYa39Ur8mmrfop7X+BuglLQ9BZH8Rzp7GlkTYR/VQyHgni1Ni8y2JqYOXSGLlATI5yi2OLOouJjmqoKSHBLiabGoh69cux72R90uqw25/GZrqQNLZXQ1dyzv/tnUQqBtOYPv3QtNPRS7ZXzHW8JbN+Z8ipGnPtJqoxlYAKWBcHp1qf++IHi/GUxfczDxuygCXGP8yRdenPWVOVw4mQYwgUXVixNLaGPnDvGK4fHrrEUIL1+PAlHtvVVIyhYkO0+uN6v6DnB+eupxJo/YRRt34p/Y8tgB3NiFtg2AnS1vwUYIxGF4ObpN+TRUv9npz+m8M71luwU00NH9XMBa3YW+wmsyhEaDsCohtx7S4uNys8924ifa2jxcogPDaFr3QtqG1koTHpPvMoog5EJTT3sq3SYTKtq1WeNOZCXPpSPRO2uJvYl0JSMMg71ZLf76GvlGJ4Jkx7b1j2o0lxQkFbPILiOHQ84N5UKPFYe1CK6DHj4bgfO+Ha+yg57K33h9vZuvXeMf5j7mNXPRGNoyiKqS1kVjU3GmszVvrGTTFp6fbCCa+1jGot12LFdEqBJrIRKdiurcVU4jEU4mKjWJhJlh5CKGoTwOEEfjcmBfB6T02AKEX11Z5Ju6WgOrgGZKSUfzeRx4PxVikhR4wXTjLqQnMUzhTv5LoH64KP0U5CCAuCh3nrKIXD6F2JRmMlTNerKuSdpZ0jrzW3PGnMhZb0gYrDNqVEAuzLd2xfFGYyvg2mkjEyN7Gs/kjG9DUFQp3HG4qR1RYarsybffTjAG8wmmA9KGNxiLamxNzFmhDTv2QUT8GFOgnHhLm2zH00RsKdtXSjsTqYoKsSyKIYNsJnWhSDAoASajvj02CfwNJD5nGIXVtheTM8GKfAI+SDUSa+aIIM78ynm9jVH3iJB9OJzK87sGnIpv7nugdrYuljtLrD9PMP1l0aVijI+NcjTNXWRpYdja1NzPOIlRh5CoLbAIFiI1CSNJ6amio2LnZuavHrhBoAgbwQKEkav3jxIq/GzukmGI3nBBdc/NoQGB8fL0kap9Pp4mMGNC4+xlBDIRAoVRpnMplnz54VAgEXGUBjF3Dg1CJCgAzFY2Nj43grqmZPC7S9kcHb7OzsxMREUTUG4YDA4kdgbGzsyZMnJelUEybPzMwsfpRBQ0CgeAiMjY39+OOPpU3jTCbz8uXL//znP8WDCSQDAosWgSdPnoyOjpYDjcmwPDU1BQ72orU2UKzgCDx58iSVSj169KisaEzI/OrVq5mZmefPn0/DBggsVgR+mpoeezo9NDp1OP6/c9Lxzp07Q0NDd+/eHRoaGhkZ+eGHH8qTxoTM8BcQWMwIzLzK/Dg1m/zh5f/s/dec9PzHP/5x8+bN27dvA43nhBtcDAgUHgGgMXljpb1wKjzAIBEQKD4CQGOgcfGtDGooMgJAY6BxkU0MxBcfAaAxofH/A5CqMy2qeGBRAAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "id": "afbf142d-d7ab-4297-8801-063e7fc74ff4",
   "metadata": {},
   "source": [
    "К сожалению, программа работала очень долго (9,5 часов) и ничего не дала :( Я оставляла ее на ночь, но она сбилась, мне пришлось ее перезапускать с утра, а потом она работала 9,5 часов и я не смогла уже больше ждать...  \n",
    "![image.png](attachment:1a128d63-9bd7-4c58-aa81-cad327627ef0.png)  \n",
    "Вот запуск этой программы на маленькой части корпуса (первых 10 предложениях):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c02f7535-d2b8-4fb3-b5bd-7375c9744ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.417910447761194\n"
     ]
    }
   ],
   "source": [
    "# Cosine distance на 10 предложениях\n",
    "\n",
    "correct_guesses = 0 # счетчик угаданных значений\n",
    "all_guesses = 0 # счетчик попыток угадать значение\n",
    "\n",
    "for sentence in corpus_wsd[:10]: # бежим по предложениям\n",
    "    for word in sentence: # внутри предложения бежим по словам\n",
    "        if word[0] != '': # рассматриваем только многозначные слова - у которых в первой ячейке что-то написано\n",
    "            sentence_string = '' # здесь будет строкой лежать контекст - всё предложение, в котором целевое слово заменено на нижнее подчеркивание\n",
    "            for word_1 in sentence:\n",
    "                if word_1 != word:\n",
    "                    sentence_string += word_1[2] + ' ' # для всех слов, кроме целевого, прибавляем их в контекст\n",
    "                else:\n",
    "                    sentence_string += '_ ' # целевое слово не добавляем, добавляем вместо него нижнее подчеркивание\n",
    "\n",
    "            # на этом этапе мы для данного слова имеем контекст, например: \"What a _ day!\" для слова word = \"beautyful\"\n",
    "            # далее будем доставать все возможные определения для этого слова и сравнивать каждое из них с контекстом:\n",
    "            \n",
    "            all_cosines = [] # здесь будет лежать список всех косинусных расстояний\n",
    "            \n",
    "            for synset in wn.synsets(word[1]): # идем по синсетам этого слова (здесь берем word[1], поскольку там лежит лемма слова)\n",
    "                new_definition = synset.definition() # вытаскиваем определения этого синсета\n",
    "                definition_emb = embed(new_definition) # делаем эмбеддинг определения\n",
    "                context_emb = embed(sentence_string) # делаем эмбеддинг контекста\n",
    "\n",
    "                # считаем косинусное расстояние:\n",
    "                \n",
    "                new_distance = cosine_distances(context_emb.reshape(1, -1), definition_emb.reshape(1, -1))\n",
    "                all_cosines.append([synset, new_distance[0][0]]) # добавили в список косинусных расстояний сам синсет + расстояние\n",
    "\n",
    "            # на этом этапе у нас есть список вида [[synset1, distance1], [synset2, distance2], ...]\n",
    "            # далее будем выискивать тот синсет / те синсеты, при котором / которых косинусное расстояние минимально\n",
    "            # важно: таких синсетов может быть несколько, и я бы хотела учесть их все, а не взять какой-то один случайный из них\n",
    "\n",
    "            min_synsets = [] # список таких синсетов\n",
    "            \n",
    "            min_cosine = all_cosines[0][1] # сюда положим предварительное минимальное значение метрики\n",
    "            \n",
    "            for cosine in all_cosines:\n",
    "                if cosine[1] == min_cosine: # если нашли что-то с таким же расстоянием, добавляем синсет в список min_synsets\n",
    "                    min_synsets.append(cosine[0])\n",
    "                elif cosine[1] < min_cosine: # если нашли что-то с расстоянием поменьше, то меняем min_cosine и полностью заменяем min_synset\n",
    "                    min_cosine = cosine[1]\n",
    "                    min_synsets = []\n",
    "                    min_synsets.append(cosine[0])\n",
    "\n",
    "            # на этом этапе мы знаем синсет(ы) с наименьшим косинусным расстоянием\n",
    "            # нам осталось сравнить эти синсеты с word[0] - то есть с тэгом многозначного слова, с которым мы работаем\n",
    "            \n",
    "            list_of_keys = [] # это список тэгов (поскольку у одного синсета может быть несколько лемм, а потому несколько тэгов)\n",
    "\n",
    "            for synset in min_synsets: # идем по всем синсетам с минимальным значением метрики\n",
    "                for lemma in synset.lemmas(): # идем по леммам каждого такого синсета\n",
    "                    list_of_keys.append(lemma.key()) # для каждой леммы вытаскиваем ее тэг\n",
    "\n",
    "            # сейчас у нас есть список тэгов данного синсета, и нам нужно понять, есть ли среди этих тэгов правильный - совпадающий с word[0]\n",
    "            \n",
    "            is_correct = 0 # пока что будем считать, что правильного тэга нет, но если найдем, поменяем значение переменной\n",
    "            \n",
    "            for k in list_of_keys:\n",
    "                if k == word[0]:\n",
    "                    is_correct += 1 # если нашли тэг, совпадающий с word[0], меняем значение переменной\n",
    "\n",
    "            if is_correct > 0: # если значение переменной is_correct поменялось, значит, мы угадали!\n",
    "                correct_guesses += 1 # добавляем единичку в счетчик угаданных значений\n",
    "\n",
    "            all_guesses += 1 # вне зависимости от правильности угадывания, увеличиваем счетчик попыток угадать значение\n",
    "\n",
    "\n",
    "accuracy = correct_guesses / all_guesses # значение accuracy\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69e8b89-f79e-4934-b391-8a60f43d5c99",
   "metadata": {},
   "source": [
    "Доля правильно угаданных значений - 0.417910447761194, то есть около 41,80%. Для сравнения, вот программа с Жаккаром на этих же десяти предложениях:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da83823a-08ab-49c2-b15f-b8d091fd1e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34106947101657686\n"
     ]
    }
   ],
   "source": [
    "# Jaccard на 10 предложениях\n",
    "\n",
    "correct_guesses = 0 # счетчик угаданных значений\n",
    "all_guesses = 0 # счетчик попыток угадать значение\n",
    "\n",
    "for sentence in corpus_wsd: # бежим по предложениям\n",
    "    for word in sentence: # внутри предложения бежим по словам\n",
    "        if word[0] != '': # рассматриваем только многозначные слова - у которых в первой ячейке что-то написано\n",
    "            sentence_string = '' # здесь будет строкой лежать контекст - всё предложение, в котором целевое слово заменено на нижнее подчеркивание\n",
    "            for word_1 in sentence:\n",
    "                if word_1 != word:\n",
    "                    sentence_string += word_1[2] + ' ' # для всех слов, кроме целевого, прибавляем их в контекст\n",
    "                else:\n",
    "                    sentence_string += '_ ' # целевое слово не добавляем, добавляем вместо него нижнее подчеркивание\n",
    "\n",
    "            # на этом этапе мы для данного слова имеем контекст, например: \"What a _ day!\" для слова word = \"beautyful\"\n",
    "            # далее будем доставать все возможные определения для этого слова и сравнивать каждое из них с контекстом:\n",
    "            \n",
    "            all_jaccards = [] # здесь будет лежать список всех Жаккаров\n",
    "            \n",
    "            for synset in wn.synsets(word[1]): # идем по синсетам этого слова (здесь берем word[1], поскольку там лежит лемма слова)\n",
    "                new_definition = synset.definition() # вытаскиваем определения этого синсета\n",
    "\n",
    "                # считаем метрику Жаккара для этого определения и контекста - контекст лежит в sentence_string:\n",
    "                \n",
    "                new_intersection = (set(new_definition.split()) & set(sentence_string.split()))\n",
    "                new_union = (set(new_definition.split()) | set(sentence_string.split()))\n",
    "                new_jaccard =  len(new_intersection) / len(new_union) # посчитали метрику Жаккара сюда\n",
    "                all_jaccards.append([synset, new_jaccard]) # добавили в список Жаккаров сам синсет + метрику Жаккара для него\n",
    "\n",
    "            # на этом этапе у нас есть список вида [[synset1, jaccard1], [synset2, jaccard2], ...]\n",
    "            # далее будем выискивать тот синсет / те синсеты, при котором / которых метрика Жаккара принимает наибольшее значение\n",
    "            # важно: таких синсетов может быть несколько, и я бы хотела учесть их все, а не взять какой-то один случайный из них\n",
    "\n",
    "            max_synsets = [] # список таких синсетов\n",
    "            \n",
    "            max_jaccard = all_jaccards[0][1] # сюда положим предварительное максимальное значение Жаккара\n",
    "            \n",
    "            for jaccard in all_jaccards:\n",
    "                if jaccard[1] == max_jaccard: # если нашли что-то с таким же Жаккаром, добавляем синсет в список max_synsets\n",
    "                    max_synsets.append(jaccard[0])\n",
    "                elif jaccard[1] > max_jaccard: # если нашли что-то с Жаккаром побольше, то меняем max_jaccard и полностью заменяем max_synset\n",
    "                    max_jaccard = jaccard[1]\n",
    "                    max_synsets = []\n",
    "                    max_synsets.append(jaccard[0])\n",
    "\n",
    "            # на этом этапе мы знаем синсет(ы) с наибольшим значением метрики Жаккара\n",
    "            # нам осталось сравнить эти синсеты с word[0] - то есть с тэгом многозначного слова, с которым мы работаем\n",
    "            \n",
    "            list_of_keys = [] # это список тэгов (поскольку у одного синсета может быть несколько лемм, а потому несколько тэгов)\n",
    "\n",
    "            for synset in max_synsets: # идем по всем синсетам с максимальным значением Жаккара\n",
    "                for lemma in synset.lemmas(): # идем по леммам каждого такого синсета\n",
    "                    list_of_keys.append(lemma.key()) # для каждой леммы вытаскиваем ее тэг\n",
    "\n",
    "            # сейчас у нас есть список тэгов данного синсета, и нам нужно понять, есть ли среди этих тэгов правильный - совпадающий с word[0]\n",
    "            \n",
    "            is_correct = 0 # пока что будем считать, что правильного тэга нет, но если найдем, поменяем значение переменной\n",
    "            \n",
    "            for k in list_of_keys:\n",
    "                if k == word[0]:\n",
    "                    is_correct += 1 # если нашли тэг, совпадающий с word[0], меняем значение переменной\n",
    "\n",
    "            if is_correct > 0: # если значение переменной is_correct поменялось, значит, мы угадали!\n",
    "                correct_guesses += 1 # добавляем единичку в счетчик угаданных значений\n",
    "\n",
    "            all_guesses += 1 # вне зависимости от правильности угадывания, увеличиваем счетчик попыток угадать значение\n",
    "\n",
    "\n",
    "accuracy = correct_guesses / all_guesses # значение accuracy\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39efde9a-af0b-4c94-bfd0-249e7054562f",
   "metadata": {},
   "source": [
    "# Задание 2 (5 балла)\n",
    "Попробуйте разные алгоритмы кластеризации на датасете - `https://github.com/nlpub/russe-wsi-kit/blob/initial/data/main/wiki-wiki/train.csv`\n",
    "\n",
    "Используйте код из семинара как основу. Используйте ARI как метрику качества.\n",
    "\n",
    "Попробуйте все 4 алгоритма кластеризации, про которые говорилось на семинаре. Для каждого из алгоритмов попробуйте настраивать гиперпараметры (посмотрите их в документации). Прогоните как минимум 5 экспериментов (не обязательно успешных) с разными параметрами на каждый алгоритме кластеризации и оцените: качество кластеризации, скорость работы, интуитивность параметров.\n",
    "\n",
    "Помимо этого также выберите 1 дополнительный алгоритм кластеризации отсюда - https://scikit-learn.org/stable/modules/clustering.html , опишите своими словами принцип его работы  и проделайте аналогичные эксперименты. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d59bef3e-1af7-4ce2-b43a-dfef282050f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузка датасета\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/nlpub/russe-wsi-kit/initial/data/main/wiki-wiki/train.csv', sep='\\t')\n",
    "grouped_df = df.groupby('word')[['word', 'context', 'gold_sense_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bec24be-d835-4bd7-a976-2aa613257ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# библиотеки для кластеризации\n",
    "\n",
    "from sklearn.cluster import KMeans, DBSCAN, AffinityPropagation, AgglomerativeClustering, BisectingKMeans\n",
    "import numpy as np\n",
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "722aa80a-19bb-4870-9d8f-824fbd730e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.054321186256441556\n"
     ]
    }
   ],
   "source": [
    "# алгоритм KMeans\n",
    "\n",
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "    X = np.zeros((len(texts), 768))\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # делим на три кластера\n",
    "    cluster = KMeans(3)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39f2fd11-64bb-418e-99c6-be1aca80fca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.058898405219530645\n"
     ]
    }
   ],
   "source": [
    "# алгоритм KMeans\n",
    "\n",
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "    X = np.zeros((len(texts), 768))\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # делим на четыре кластера\n",
    "    cluster = KMeans(4)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b34165b2-1e57-4992-8a2f-49377d5012dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04381089488387982\n"
     ]
    }
   ],
   "source": [
    "# алгоритм KMeans\n",
    "\n",
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "    X = np.zeros((len(texts), 768))\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # делим на пять кластеров\n",
    "    cluster = KMeans(5)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f3affcc-9b25-407a-b89c-0e5bc245f2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07195226820006335\n"
     ]
    }
   ],
   "source": [
    "# алгоритм KMeans\n",
    "\n",
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "    X = np.zeros((len(texts), 768))\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # делим на шесть кластеров\n",
    "    cluster = KMeans(6)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46a19d76-f2de-4f9b-a8d9-d75ca3cf049b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03580086941376679\n"
     ]
    }
   ],
   "source": [
    "# алгоритм KMeans\n",
    "\n",
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "    X = np.zeros((len(texts), 768))\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # делим на семь кластеров\n",
    "    cluster = KMeans(7)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17f21171-73f5-489e-9250-44a3fbfde20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001053019960000099\n"
     ]
    }
   ],
   "source": [
    "# алгоритм DBSCAN\n",
    "\n",
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "    X = np.zeros((len(texts), 768))\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # настраиваем параметры\n",
    "    cluster = DBSCAN(min_samples = 1, eps = 0.1)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0f7d9e8-c422-44c6-930d-52d1c85cc735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0021290615824144776\n"
     ]
    }
   ],
   "source": [
    "# алгоритм DBSCAN\n",
    "\n",
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "    X = np.zeros((len(texts), 768))\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # настраиваем параметры\n",
    "    cluster = DBSCAN(min_samples = 3, eps = 0.1)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "986bec82-87dd-4862-96e6-2eb8f174b253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00443342207693399\n"
     ]
    }
   ],
   "source": [
    "# алгоритм DBSCAN\n",
    "\n",
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "    X = np.zeros((len(texts), 768))\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # настраиваем параметры\n",
    "    cluster = DBSCAN(min_samples = 1, eps = 0.2)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9178796-68bb-45e8-9ff8-88bbd92b51af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.046628449087722304\n"
     ]
    }
   ],
   "source": [
    "# алгоритм DBSCAN\n",
    "\n",
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "    X = np.zeros((len(texts), 768))\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # настраиваем параметры\n",
    "    cluster = DBSCAN(min_samples = 1, eps = 0.5)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02dfe277-118a-4508-99ad-f4a4262ee8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0016975060994403033\n"
     ]
    }
   ],
   "source": [
    "# алгоритм DBSCAN\n",
    "\n",
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "    X = np.zeros((len(texts), 768))\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # настраиваем параметры\n",
    "    cluster = DBSCAN(min_samples = 1, eps = 0.8)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4fd94f5c-a843-47fe-ab0e-9766bdbdd344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05297560306165972\n"
     ]
    }
   ],
   "source": [
    "# алгоритм Affinity Propagation\n",
    "\n",
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "    X = np.zeros((len(texts), 768))\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # настраиваем параметры\n",
    "    cluster = AffinityPropagation(damping = 0.9)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0949213a-4982-46c5-ae89-abcb65b318a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04154515818974152\n"
     ]
    }
   ],
   "source": [
    "# алгоритм Affinity Propagation\n",
    "\n",
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "    X = np.zeros((len(texts), 768))\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # настраиваем параметры\n",
    "    cluster = AffinityPropagation(damping = 0.8)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b5b23212-5629-428d-b661-2afca77f49a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.042740969848549505\n"
     ]
    }
   ],
   "source": [
    "# алгоритм Affinity Propagation\n",
    "\n",
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "    X = np.zeros((len(texts), 768))\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # настраиваем параметры\n",
    "    cluster = AffinityPropagation(damping = 0.5)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a93debbd-f40c-496c-b560-7cefd1ffe20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\irina\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\cluster\\_affinity_propagation.py:164: ConvergenceWarning: Affinity propagation did not converge and this model will not have any cluster centers.\n",
      "  warnings.warn(\n",
      "c:\\users\\irina\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\cluster\\_affinity_propagation.py:142: ConvergenceWarning: Affinity propagation did not converge, this model may return degenerate cluster centers and labels.\n",
      "  warnings.warn(\n",
      "c:\\users\\irina\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\cluster\\_affinity_propagation.py:142: ConvergenceWarning: Affinity propagation did not converge, this model may return degenerate cluster centers and labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02111210700136976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\irina\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\cluster\\_affinity_propagation.py:164: ConvergenceWarning: Affinity propagation did not converge and this model will not have any cluster centers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# алгоритм Affinity Propagation\n",
    "\n",
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "    X = np.zeros((len(texts), 768))\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # настраиваем параметры\n",
    "    cluster = AffinityPropagation(damping = 0.9, max_iter = 10)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "57a05f93-94e3-4c5a-ab3f-eeb007e048b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.021849897349496914\n"
     ]
    }
   ],
   "source": [
    "# алгоритм Affinity Propagation\n",
    "\n",
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "    X = np.zeros((len(texts), 768))\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # настраиваем параметры\n",
    "    cluster = AffinityPropagation(damping = 0.9, convergence_iter = 2)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed9241c8-048a-486a-a3fe-9acc4e2b91ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.034738071269679226\n"
     ]
    }
   ],
   "source": [
    "# алгоритм Agglomerative Clustering\n",
    "\n",
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "    X = np.zeros((len(texts), 768))\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # настраиваем параметры\n",
    "    cluster = AgglomerativeClustering(3)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b8069cc-29a3-4948-a8df-fa0f2c5673de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.038617257915792846\n"
     ]
    }
   ],
   "source": [
    "# алгоритм Agglomerative Clustering\n",
    "\n",
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "    X = np.zeros((len(texts), 768))\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # настраиваем параметры\n",
    "    cluster = AgglomerativeClustering(4)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8cbafbe-6a8f-4958-9d68-653b0a1046cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03957581346650506\n"
     ]
    }
   ],
   "source": [
    "# алгоритм Agglomerative Clustering\n",
    "\n",
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "    X = np.zeros((len(texts), 768))\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # настраиваем параметры\n",
    "    cluster = AgglomerativeClustering(6)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "835fc092-f4a0-4414-81ac-cf208ebe8cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.057641599566266404\n"
     ]
    }
   ],
   "source": [
    "# алгоритм Agglomerative Clustering\n",
    "\n",
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "    X = np.zeros((len(texts), 768))\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # настраиваем параметры\n",
    "    cluster = AgglomerativeClustering(8)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2cbf2b5e-dc9c-4c11-a0b2-288f1b1a9051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.011976265536517934\n"
     ]
    }
   ],
   "source": [
    "# алгоритм Agglomerative Clustering\n",
    "\n",
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "    X = np.zeros((len(texts), 768))\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # настраиваем параметры\n",
    "    cluster = AgglomerativeClustering(2)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef579193-8404-4873-9e80-c05baa63bbd4",
   "metadata": {},
   "source": [
    "Дополнительный алгоритм - Bisecting K-Means. Он похож на обычный K-Means, но только он итеративный: в нем центры кластеров выбираются не сразу все рандомно, а постепенно, в зависимости от уже определенных ранее кластеров.  \n",
    "Поскольку алгоритм K-Means показал наилучший результат на шести кластерах, а алгоритм Agglomerative Clustering - на восьми кластерах, я и здесь сразу проведу эксперимент на шести и на восьми кластерах, а также, для сравнения, на четырех."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66287fe9-c248-4ef8-b810-671905c846c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04058406948714827\n"
     ]
    }
   ],
   "source": [
    "# алгоритм BisectingKMeans\n",
    "\n",
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "    X = np.zeros((len(texts), 768))\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # делим на шесть кластеров\n",
    "    cluster = BisectingKMeans(6)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8148836d-8da6-4daa-9f2a-86545b29b538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03943342693143612\n"
     ]
    }
   ],
   "source": [
    "# алгоритм BisectingKMeans\n",
    "\n",
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "    X = np.zeros((len(texts), 768))\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # делим на четыре кластера\n",
    "    cluster = BisectingKMeans(8)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d77ffa8-e576-4079-9608-cf34ca31c5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09886769997985852\n"
     ]
    }
   ],
   "source": [
    "# алгоритм BisectingKMeans\n",
    "\n",
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "    X = np.zeros((len(texts), 768))\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # делим на четыре кластера\n",
    "    cluster = BisectingKMeans(4)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
